{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_meters</th>\n",
       "      <th>popularity</th>\n",
       "      <th>rating</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>bikes_available</th>\n",
       "      <th>bikes_in_use</th>\n",
       "      <th>total_bikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45889.000000</td>\n",
       "      <td>15384.000000</td>\n",
       "      <td>5985.000000</td>\n",
       "      <td>45889.000000</td>\n",
       "      <td>45889.000000</td>\n",
       "      <td>45889.000000</td>\n",
       "      <td>45889.000000</td>\n",
       "      <td>45889.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.389331</td>\n",
       "      <td>0.696245</td>\n",
       "      <td>7.148204</td>\n",
       "      <td>52.510455</td>\n",
       "      <td>13.389849</td>\n",
       "      <td>1.847763</td>\n",
       "      <td>3.354006</td>\n",
       "      <td>5.156399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.338322</td>\n",
       "      <td>0.306045</td>\n",
       "      <td>1.035183</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.053063</td>\n",
       "      <td>2.352336</td>\n",
       "      <td>2.439783</td>\n",
       "      <td>3.252509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>52.438212</td>\n",
       "      <td>13.228195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>52.494314</td>\n",
       "      <td>13.346871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.854558</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>52.510176</td>\n",
       "      <td>13.396425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.944794</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>52.527044</td>\n",
       "      <td>13.428717</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>52.637125</td>\n",
       "      <td>13.607622</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_meters    popularity       rating           lat          long  \\\n",
       "count     45889.000000  15384.000000  5985.000000  45889.000000  45889.000000   \n",
       "mean         60.389331      0.696245     7.148204     52.510455     13.389849   \n",
       "std          25.338322      0.306045     1.035183      0.025355      0.053063   \n",
       "min           0.000000      0.000088     4.100000     52.438212     13.228195   \n",
       "25%          41.000000      0.453704     6.400000     52.494314     13.346871   \n",
       "50%          61.000000      0.854558     7.200000     52.510176     13.396425   \n",
       "75%          80.000000      0.944794     7.900000     52.527044     13.428717   \n",
       "max         200.000000      0.999940     9.500000     52.637125     13.607622   \n",
       "\n",
       "       bikes_available  bikes_in_use   total_bikes  \n",
       "count     45889.000000  45889.000000  45889.000000  \n",
       "mean          1.847763      3.354006      5.156399  \n",
       "std           2.352336      2.439783      3.252509  \n",
       "min           0.000000      0.000000      2.000000  \n",
       "25%           0.000000      2.000000      4.000000  \n",
       "50%           1.000000      3.000000      4.000000  \n",
       "75%           3.000000      4.000000      4.000000  \n",
       "max          17.000000     22.000000     33.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_poi_df = pd.read_csv('../data/joined_bikes.csv')\n",
    "bike_poi_df = bike_poi_df.drop('Unnamed: 0',axis=1)\n",
    "bike_poi_df = bike_poi_df[bike_poi_df['distance_meters'] < 201]\n",
    "bike_poi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to make a model that demonstrates a relationship between the number of bikes at a station and the charactaristics\n",
    "# of the POIs there.\n",
    "import seaborn as sns\n",
    "sns.heatmap(bike_poi_df.corr())\n",
    "# Looking at my sad heatmap again I think I'll have to 'unpack' the data in the 'category' column to be binary.\n",
    "# For example I could add a columns called 'near_restaurant'/'near_cafe'/'residential'/etc where 0 is False and 1 is True.\n",
    "# This would be tied to origin like total_bikes is. One issue that immediatley pops into mind with this is the 'poi density' of \n",
    "# different origins. Some have 50 POIs tied to them. Some have 5. I'll have to group by origin, total_bikes, and the categorical\n",
    "# columns to void this. Essentially unjoining the two tables.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_poi_df['category'] = bike_poi_df['category'].str.lower()\n",
    "# Just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business and professional services    2985\n",
       "clothing store                        1154\n",
       "attorney / law office                 1066\n",
       "hair salon                             952\n",
       "café                                   865\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_poi_df['category'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           declare new column               do a lambda transform so we can evaluate every category in each grouped origin\n",
    "bike_poi_df['near_restaurant'] = bike_poi_df.groupby('origin')['category'].transform(\n",
    "    lambda x: x.str.contains('restaurant').any()\n",
    "    ).map({True: 1, False: 0})\n",
    "#   set the value of the new column\n",
    "\n",
    "\n",
    "# Run this to get a feel of the impact this new column will have. for example take technology business at 384 occurences.\n",
    "bike_poi_df.groupby('origin')['category'].transform(\n",
    "    lambda x: x.str.contains('business').any()\n",
    "    ).value_counts()\n",
    "# if we try this chunk of code with contains('business') it's way too vague and will change nearly 40k rows.\n",
    "# if we try it with ('technology') it only changes 10k rows.\n",
    "# we can also do to see exactly what categories it's picking up.\n",
    "bike_poi_df[bike_poi_df['category'].str.contains('technology',na=False)].category.value_counts()\n",
    "# In this case it's only referencing 'technology business' I think it's a good add.\n",
    "\n",
    "bike_poi_df['near_tech'] = bike_poi_df.groupby('origin')['category'].transform(\n",
    "    lambda x: x.str.contains('technology').any()\n",
    "    ).map({True: 1, False: 0})\n",
    "bike_poi_df.head(1)\n",
    "\n",
    "# I am going to keep testing and adding these binary columns. I'll just print the final database in the cell below to avoid clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>distance_meters</th>\n",
       "      <th>category</th>\n",
       "      <th>popularity</th>\n",
       "      <th>rating</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>origin</th>\n",
       "      <th>bikes_available</th>\n",
       "      <th>bikes_in_use</th>\n",
       "      <th>...</th>\n",
       "      <th>near_language_school</th>\n",
       "      <th>near_music_school</th>\n",
       "      <th>near_nursery_school</th>\n",
       "      <th>near_driving_school</th>\n",
       "      <th>near_publisher</th>\n",
       "      <th>near_pub</th>\n",
       "      <th>near_bookstore</th>\n",
       "      <th>near_library</th>\n",
       "      <th>near_bridge</th>\n",
       "      <th>near_landmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Curry Wolf</td>\n",
       "      <td>29.0</td>\n",
       "      <td>fast food restaurant</td>\n",
       "      <td>0.903641</td>\n",
       "      <td>7.8</td>\n",
       "      <td>52.503902</td>\n",
       "      <td>13.335662</td>\n",
       "      <td>52.504157,13.335328</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  distance_meters              category  popularity  rating  \\\n",
       "0  Curry Wolf             29.0  fast food restaurant    0.903641     7.8   \n",
       "\n",
       "         lat       long               origin  bikes_available  bikes_in_use  \\\n",
       "0  52.503902  13.335662  52.504157,13.335328                4             0   \n",
       "\n",
       "   ...  near_language_school  near_music_school  near_nursery_school  \\\n",
       "0  ...                     0                  0                    0   \n",
       "\n",
       "   near_driving_school  near_publisher  near_pub  near_bookstore  \\\n",
       "0                    0               0         0               0   \n",
       "\n",
       "   near_library  near_bridge  near_landmark  \n",
       "0             0            0              0  \n",
       "\n",
       "[1 rows x 92 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "bike_poi_df = pd.read_csv('../data/joined_bikes_expanded.csv') # If you reload the file run this.\n",
    "bike_poi_df = bike_poi_df.drop('Unnamed: 0',axis=1)\n",
    "bike_poi_df.head(1)\n",
    "# I zoned out and we're at nearly 100 columns now...\n",
    "# bike_poi_df.to_csv('../data/joined_bikes_expanded.csv') save before I lose my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(bike_poi_df.corr())\n",
    "# needles to say, this is too chaotic to digest. It does seem like there's more connections though so that's good.\n",
    "\n",
    "# total bikes will be dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab columns relevant to station\n",
    "stations = bike_poi_df[bike_poi_df.columns[7:].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_stations = stations.drop_duplicates()\n",
    "distinct_stations = distinct_stations.drop(['bikes_in_use','bikes_available'],axis=1)\n",
    "y = distinct_stations['total_bikes']\n",
    "distinct_stations = distinct_stations.drop(['total_bikes','origin','near_landmark'],axis=1)\n",
    "X_tmp = distinct_stations[distinct_stations.columns.to_list()]\n",
    "col = X_tmp.columns\n",
    "X = sm.add_constant(X_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimination(X,y,col):\n",
    "\n",
    "    while len(col)>0 :\n",
    "        model=sm.OLS(y,X[col])\n",
    "        result=model.fit()\n",
    "        largest_pvalue=round(result.pvalues,3).nlargest(1)\n",
    "        if largest_pvalue[0]<(0.0045):\n",
    "            return result\n",
    "            break\n",
    "        else:\n",
    "            col=col.drop(largest_pvalue.index)\n",
    "            print('dropped',largest_pvalue)\n",
    "\n",
    "result = elimination(X,y,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_stations = stations.drop_duplicates()\n",
    "distinct_stations = distinct_stations.drop(['bikes_in_use','bikes_available'],axis=1)\n",
    "y = distinct_stations['total_bikes']\n",
    "distinct_stations = distinct_stations.drop(['total_bikes','origin','near_landmark'],axis=1)\n",
    "indep = distinct_stations[distinct_stations.columns.to_list()]\n",
    "X = [sm.add_constant(indep[column]) for column in indep.columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['near_website_designer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a challenge I took Jeremy's forward selection code from his lecture and automated it.\n",
    "\n",
    "# y is still the dependent var.\n",
    "# X this time is a list containing each column in x that has been turned into a df with a constant.\n",
    "def addition(X,y):\n",
    "    df_list = list()\n",
    "    # List of columns that have been selected.\n",
    "\n",
    "    best_r = -1\n",
    "    r_val = 0\n",
    "    # while params\n",
    "\n",
    "    remaining_var = distinct_stations\n",
    "    # filtered dataframe that does not contain selected columns.\n",
    "    while(best_r < r_val):\n",
    "\n",
    "        Models = [sm.OLS(y,x) for x in X] #list of models\n",
    "        Results = [model.fit() for model in Models] #list of results\n",
    "        # print(len(Results))\n",
    "        Adj_Rsquared = [results.rsquared_adj for results in Results] #list of rsquared\n",
    "\n",
    "        # Find highest r value and the column it came from.\n",
    "        tmp_dict = dict()\n",
    "        for i in range(len(Adj_Rsquared)):\n",
    "            tmp_dict[remaining_var.columns[i]] = Adj_Rsquared[i]\n",
    "\n",
    "        tmp_df = pd.DataFrame.from_dict(tmp_dict.items())\n",
    "        r_col = tmp_df.max().values[0]\n",
    "        r_val = tmp_df.max().values[1]\n",
    "        # print(r_val)\n",
    "        # print(r_col)\n",
    "\n",
    "        # Add our column to the list.\n",
    "        df_list.append(r_col)\n",
    "\n",
    "        # drop the selected column from the pool.\n",
    "        remaining_var = remaining_var.drop(r_col,axis=1)\n",
    "\n",
    "        # columns that we have selected and will be brought forward.\n",
    "        included_df =  distinct_stations[df_list]\n",
    "\n",
    "        # X is now a list of dataframes of format (constant, selected columns, remaining column)\n",
    "        # Each remaining column that needs to be tested will have a place in this list under the above format.\n",
    "        X = [sm.add_constant(pd.merge(included_df,remaining_var[column], right_index = True, left_index = True)) for column in remaining_var.columns]\n",
    "        # print(X[0]) uncomment this to better visualize it.\n",
    "        best_r = r_val\n",
    "        \n",
    "        \n",
    "    return df_list\n",
    "        \n",
    "\n",
    "selection = addition(X,y)\n",
    "selection\n",
    "\n",
    "# It was bittersweet to see this run as intended but also immediately return it's best selection..\n",
    "# You can make the while loop run forever and print the r_Val and r_col to get an idea of how it's 'thinking'\n",
    "# This will throw an error when there's nothing else to select so you won't have to restart your kernel.\n",
    "\n",
    "# I think there could be application for this but one takeaway is that forward selection is not the way to go when you have a ton of\n",
    "# variables to filter through.\n",
    "# If I had more time I would look at making variations of this for example:\n",
    "#   a function that does many 'seeded' runs of this and returns the best one. (start at random column instead of best column first.. you may as well \n",
    "#   try a start off every variable and return the best option at that point though.)\n",
    "#   Run the selection that is returned from the elimation function. We already have a decent r value there and this could be a quick test to see if it could be improved\n",
    "#   You could also try run the selection minus the most recently added one.\n",
    "#   I think this has potential to produce decent results as the elimination is purely looking at pval so feeding it so forward selection then gives a final check to\n",
    "#   see if any better r vals can be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide model output and an interpretation of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>total_bikes</td>   <th>  R-squared (uncentered):</th>      <td>   0.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   222.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 29 Jan 2023</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:41:35</td>     <th>  Log-Likelihood:    </th>          <td> -4823.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1771</td>      <th>  AIC:               </th>          <td>   9671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1759</td>      <th>  BIC:               </th>          <td>   9736.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_restaurant</th>      <td>    1.5946</td> <td>    0.187</td> <td>    8.528</td> <td> 0.000</td> <td>    1.228</td> <td>    1.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_prof_service</th>    <td>    2.4438</td> <td>    0.165</td> <td>   14.780</td> <td> 0.000</td> <td>    2.119</td> <td>    2.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_bakery</th>          <td>    0.8151</td> <td>    0.201</td> <td>    4.059</td> <td> 0.000</td> <td>    0.421</td> <td>    1.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_bar</th>             <td>    0.7035</td> <td>    0.196</td> <td>    3.589</td> <td> 0.000</td> <td>    0.319</td> <td>    1.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_organization</th>    <td>    1.3876</td> <td>    0.222</td> <td>    6.246</td> <td> 0.000</td> <td>    0.952</td> <td>    1.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_park</th>            <td>    1.1595</td> <td>    0.378</td> <td>    3.070</td> <td> 0.002</td> <td>    0.419</td> <td>    1.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_museum</th>          <td>    1.1550</td> <td>    0.370</td> <td>    3.118</td> <td> 0.002</td> <td>    0.429</td> <td>    1.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_prof_cleaner</th>    <td>    0.9313</td> <td>    0.259</td> <td>    3.596</td> <td> 0.000</td> <td>    0.423</td> <td>    1.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_computer_repair</th> <td>    0.8232</td> <td>    0.277</td> <td>    2.971</td> <td> 0.003</td> <td>    0.280</td> <td>    1.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_playground</th>      <td>    1.0397</td> <td>    0.308</td> <td>    3.380</td> <td> 0.001</td> <td>    0.436</td> <td>    1.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_plaza</th>           <td>    1.2112</td> <td>    0.282</td> <td>    4.296</td> <td> 0.000</td> <td>    0.658</td> <td>    1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_bridge</th>          <td>    1.8387</td> <td>    0.398</td> <td>    4.619</td> <td> 0.000</td> <td>    1.058</td> <td>    2.619</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>714.302</td> <th>  Durbin-Watson:     </th> <td>   1.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3739.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.842</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 9.092</td>  <th>  Cond. No.          </th> <td>    5.46</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:            total_bikes   R-squared (uncentered):                   0.603\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.600\n",
       "Method:                 Least Squares   F-statistic:                              222.4\n",
       "Date:                Sun, 29 Jan 2023   Prob (F-statistic):                        0.00\n",
       "Time:                        12:41:35   Log-Likelihood:                         -4823.4\n",
       "No. Observations:                1771   AIC:                                      9671.\n",
       "Df Residuals:                    1759   BIC:                                      9736.\n",
       "Df Model:                          12                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "========================================================================================\n",
       "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "near_restaurant          1.5946      0.187      8.528      0.000       1.228       1.961\n",
       "near_prof_service        2.4438      0.165     14.780      0.000       2.119       2.768\n",
       "near_bakery              0.8151      0.201      4.059      0.000       0.421       1.209\n",
       "near_bar                 0.7035      0.196      3.589      0.000       0.319       1.088\n",
       "near_organization        1.3876      0.222      6.246      0.000       0.952       1.823\n",
       "near_park                1.1595      0.378      3.070      0.002       0.419       1.900\n",
       "near_museum              1.1550      0.370      3.118      0.002       0.429       1.881\n",
       "near_prof_cleaner        0.9313      0.259      3.596      0.000       0.423       1.439\n",
       "near_computer_repair     0.8232      0.277      2.971      0.003       0.280       1.366\n",
       "near_playground          1.0397      0.308      3.380      0.001       0.436       1.643\n",
       "near_plaza               1.2112      0.282      4.296      0.000       0.658       1.764\n",
       "near_bridge              1.8387      0.398      4.619      0.000       1.058       2.619\n",
       "==============================================================================\n",
       "Omnibus:                      714.302   Durbin-Watson:                   1.100\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3739.905\n",
       "Skew:                           1.842   Prob(JB):                         0.00\n",
       "Kurtosis:                       9.092   Cond. No.                         5.46\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-value\n",
    "The r values indicate that the model is unable to explain 40% of the variance of the dependent variable.\n",
    "\n",
    "Online it seems like the consensus is that values of .6 plus display a moderate correlation.\n",
    "\n",
    "I think this makes sense. Take this plot from my eda showing total bikes represented by the size of the marker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![failed to load](https://cdn.discordapp.com/attachments/1063653051602321462/1069354545005731870/image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heart of Berlin there's a ton of these stations with 4-6 total bikes. Here and there there's stations with ~20-30 total_bikes. Due to the tight grouping, even with a relatively small radius they're bound to have very similiar poi's associated with them.\n",
    "\n",
    "I think it's these, for lack of a better word, outliers that are greatly responsible for our r values.\n",
    "\n",
    "I Googled 'regression model that's for outliers than OLS' and the wiki for robust regression popped up so I searched for the statsmodel application of it.\n",
    "\n",
    "I tried it out and found the summary doesn't go quite as deep as an OLS summary. I noticed it did use some more independent variables though. I think to properly compare them you'd have to do a regression plot and so on but I'll spare you from another long tangent.\n",
    "\n",
    "### F-test\n",
    "The p-value for the f-test is very low so we know our model is a much better fit than a empty model.\n",
    "### Log likelihood\n",
    "We don't have another model to compare at the moment so this is more or less meaningless.\n",
    "\n",
    "BIC and AIC is also relative so we'll ignore it.\n",
    "### Coefficients\n",
    "near_prof_service is one I didn't expect to have such a high coef. Let's scatter plot it to see where these are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![failed to load](https://cdn.discordapp.com/attachments/1063653051602321462/1069364318803607674/image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a ton of them. They're by a lot of, maybe even almost all of the lower total_bikes stations but they're also buy all of the stations with lots of total_bikes.\n",
    "\n",
    "It's hard to tell exactly how this is effecting my model but my model seems to really like being near them.\n",
    "\n",
    "Otherwise the first thing I noticed was bridge, park, playground and their significance. This aligns with the patterns I noticed when I first started using these maps where the most popular and biggest bike stations are by parks,nature, or bike paths of some sort. If our API had better means of detecting these locations these correlations would be even stronger.\n",
    "\n",
    "Also I initially expected bars would have a stronger correlation to bike sharing with drunk driving and all. I'm not sure why that popped into mind considering biking in heavy traffic while drunk is a lot more dangerous than just getting a cab haha."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![failed to load](https://cdn.discordapp.com/attachments/1063653051602321462/1069388793976717392/image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a residual plot of the model. Looking at its worst point the jump from 4 to 26 bikes under the same criteria is pretty brutal. I do still think it makes note of valuable patterns in the data but I don't think it's going to be used as a prediction model too well.\n",
    "\n",
    "I played around for a while fine tuning the independent categories being passed through but kept getting worse r values and about the same residual plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Remark\n",
    "The first thing I'll say is university campuses are the cash cows of the bike sharing world. Buch campus being a glowing example. By their dorms and shopping center they have high volume bike stations that are in constant very high demand, by their classes they have more bike stations also high in demand and right on campus they have a massive bike sharing station on the edge of the park so all the young people can go out and bike around the trails. Every station on that campus is used to its full potential. A lot of the students are active, a lot of the students don't have a lot of money or a car, and a lot of the students do fun things. Needless to say if a company has the oportunity to put their bikesharing stations on and around a campus they should take it over every other opportunity.\n",
    "\n",
    "However if you want your stations to collect dust you can put small capacity stations on every block of the inner city. Do they make the city more bike accessible? Sure. Do they put a healthy return into the pocket of a private company? No. Leave it to the government. Locals who cycle likely have their own bikes, and tourists are far more likely to rent a bike and cycle around the waterfront, a beautiful park, or an old city shopping plaza designed for foot traffic than do circles around your apartment complex. Do these stations get used? yes but if you're looking for a station that can justify having a large capacity and utilize the majority of that capacity nearly every day these aren't the types of stations you want. If you set up a bunch of these stations you're also likely to have some that are prone to vandalism/theft/misuse which cuts further into your profits.\n",
    "\n",
    "It's worth noting that 'tourist trap' places like landmarks and famous spots will naturaly be more active during tourist season. A more consistent activity and more use from locals would could from setting up stations by parks/bike paths.\n",
    "\n",
    "In summary: campuses are your ideal first choice, nature, walkways, landmarks, foot traffic plazas come next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you turn the regression model into a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bikes</th>\n",
       "      <th>near_restaurant</th>\n",
       "      <th>near_tech</th>\n",
       "      <th>near_grocery</th>\n",
       "      <th>near_prof_service</th>\n",
       "      <th>near_clothing</th>\n",
       "      <th>near_consulting</th>\n",
       "      <th>near_attorney</th>\n",
       "      <th>near_hair_salon</th>\n",
       "      <th>near_nail_salon</th>\n",
       "      <th>...</th>\n",
       "      <th>near_language_school</th>\n",
       "      <th>near_music_school</th>\n",
       "      <th>near_nursery_school</th>\n",
       "      <th>near_driving_school</th>\n",
       "      <th>near_publisher</th>\n",
       "      <th>near_pub</th>\n",
       "      <th>near_bookstore</th>\n",
       "      <th>near_library</th>\n",
       "      <th>near_bridge</th>\n",
       "      <th>near_landmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>1771.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.032185</td>\n",
       "      <td>0.551666</td>\n",
       "      <td>0.163749</td>\n",
       "      <td>0.289102</td>\n",
       "      <td>0.674195</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.280632</td>\n",
       "      <td>0.333710</td>\n",
       "      <td>0.060418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.074534</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.050254</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.980840</td>\n",
       "      <td>0.497464</td>\n",
       "      <td>0.370152</td>\n",
       "      <td>0.453474</td>\n",
       "      <td>0.468807</td>\n",
       "      <td>0.451882</td>\n",
       "      <td>0.362273</td>\n",
       "      <td>0.449435</td>\n",
       "      <td>0.471671</td>\n",
       "      <td>0.238327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082059</td>\n",
       "      <td>0.074952</td>\n",
       "      <td>0.058124</td>\n",
       "      <td>0.053074</td>\n",
       "      <td>0.297736</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.325370</td>\n",
       "      <td>0.053074</td>\n",
       "      <td>0.218531</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_bikes  near_restaurant    near_tech  near_grocery  \\\n",
       "count  1771.000000      1771.000000  1771.000000   1771.000000   \n",
       "mean      5.032185         0.551666     0.163749      0.289102   \n",
       "std       2.980840         0.497464     0.370152      0.453474   \n",
       "min       2.000000         0.000000     0.000000      0.000000   \n",
       "25%       4.000000         0.000000     0.000000      0.000000   \n",
       "50%       4.000000         1.000000     0.000000      0.000000   \n",
       "75%       4.000000         1.000000     0.000000      1.000000   \n",
       "max      33.000000         1.000000     1.000000      1.000000   \n",
       "\n",
       "       near_prof_service  near_clothing  near_consulting  near_attorney  \\\n",
       "count        1771.000000    1771.000000      1771.000000    1771.000000   \n",
       "mean            0.674195       0.285714         0.155280       0.280632   \n",
       "std             0.468807       0.451882         0.362273       0.449435   \n",
       "min             0.000000       0.000000         0.000000       0.000000   \n",
       "25%             0.000000       0.000000         0.000000       0.000000   \n",
       "50%             1.000000       0.000000         0.000000       0.000000   \n",
       "75%             1.000000       1.000000         0.000000       1.000000   \n",
       "max             1.000000       1.000000         1.000000       1.000000   \n",
       "\n",
       "       near_hair_salon  near_nail_salon  ...  near_language_school  \\\n",
       "count      1771.000000      1771.000000  ...           1771.000000   \n",
       "mean          0.333710         0.060418  ...              0.006776   \n",
       "std           0.471671         0.238327  ...              0.082059   \n",
       "min           0.000000         0.000000  ...              0.000000   \n",
       "25%           0.000000         0.000000  ...              0.000000   \n",
       "50%           0.000000         0.000000  ...              0.000000   \n",
       "75%           1.000000         0.000000  ...              0.000000   \n",
       "max           1.000000         1.000000  ...              1.000000   \n",
       "\n",
       "       near_music_school  near_nursery_school  near_driving_school  \\\n",
       "count        1771.000000          1771.000000          1771.000000   \n",
       "mean            0.005647             0.003388             0.002823   \n",
       "std             0.074952             0.058124             0.053074   \n",
       "min             0.000000             0.000000             0.000000   \n",
       "25%             0.000000             0.000000             0.000000   \n",
       "50%             0.000000             0.000000             0.000000   \n",
       "75%             0.000000             0.000000             0.000000   \n",
       "max             1.000000             1.000000             1.000000   \n",
       "\n",
       "       near_publisher     near_pub  near_bookstore  near_library  near_bridge  \\\n",
       "count     1771.000000  1771.000000     1771.000000   1771.000000  1771.000000   \n",
       "mean         0.098250     0.074534        0.120271      0.002823     0.050254   \n",
       "std          0.297736     0.262712        0.325370      0.053074     0.218531   \n",
       "min          0.000000     0.000000        0.000000      0.000000     0.000000   \n",
       "25%          0.000000     0.000000        0.000000      0.000000     0.000000   \n",
       "50%          0.000000     0.000000        0.000000      0.000000     0.000000   \n",
       "75%          0.000000     0.000000        0.000000      0.000000     0.000000   \n",
       "max          1.000000     1.000000        1.000000      1.000000     1.000000   \n",
       "\n",
       "       near_landmark  \n",
       "count         1771.0  \n",
       "mean             0.0  \n",
       "std              0.0  \n",
       "min              0.0  \n",
       "25%              0.0  \n",
       "50%              0.0  \n",
       "75%              0.0  \n",
       "max              0.0  \n",
       "\n",
       "[8 rows x 82 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a column called 'many_bikes' with 0 being no and 1 being yes.\n",
    "binary_df = stations.drop_duplicates()\n",
    "binary_df = binary_df.drop(['bikes_in_use','bikes_available'],axis=1)\n",
    "# we know the majority of values in total_bikes is 4 and looking at the percentiles even at 75% total_bikes is still at 4\n",
    "# For this reason I think we can consider anything above 4 to be large.\n",
    "binary_df.head(1)\n",
    "binary_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column in one pass using lambda transform with map again.\n",
    "binary_df['many_bikes'] = binary_df['total_bikes'].transform(\n",
    "    lambda x: x > 4).map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bikes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many_bikes</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_bikes\n",
       "many_bikes             \n",
       "0                  1455\n",
       "1                   316"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_df[['many_bikes','total_bikes']].groupby(by='many_bikes').count()\n",
    "# Everything looks good so lets make a logistic regression model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = binary_df['many_bikes']\n",
    "binary_df = binary_df.drop(['total_bikes','many_bikes','origin','near_landmark'],axis=1)\n",
    "X_tmp = binary_df[binary_df.columns.to_list()]\n",
    "col = X_tmp.columns\n",
    "X = sm.add_constant(X_tmp)\n",
    "def elimination(X,y,col):\n",
    "\n",
    "    while len(col)>0 :\n",
    "        model=sm.Logit(y,X[col])\n",
    "        result=model.fit()\n",
    "        largest_pvalue=round(result.pvalues,3).nlargest(1)\n",
    "        if largest_pvalue[0]<(0.0045):\n",
    "            return result\n",
    "            break\n",
    "        else:\n",
    "            col=col.drop(largest_pvalue.index)\n",
    "            print('dropped',largest_pvalue)\n",
    "\n",
    "log_result = elimination(X,y,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>many_bikes</td>    <th>  No. Observations:  </th>  <td>  1771</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1766</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 29 Jan 2023</td> <th>  Pseudo R-squ.:     </th> <td>-0.08839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:37:21</td>     <th>  Log-Likelihood:    </th> <td> -904.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -830.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_restaurant</th>   <td>   -0.4756</td> <td>    0.111</td> <td>   -4.281</td> <td> 0.000</td> <td>   -0.693</td> <td>   -0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_grocery</th>      <td>   -0.5757</td> <td>    0.141</td> <td>   -4.090</td> <td> 0.000</td> <td>   -0.852</td> <td>   -0.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_prof_service</th> <td>   -0.9162</td> <td>    0.102</td> <td>   -8.999</td> <td> 0.000</td> <td>   -1.116</td> <td>   -0.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_firm</th>         <td>   -0.5322</td> <td>    0.174</td> <td>   -3.066</td> <td> 0.002</td> <td>   -0.872</td> <td>   -0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_accountant</th>   <td>   -0.5649</td> <td>    0.185</td> <td>   -3.056</td> <td> 0.002</td> <td>   -0.927</td> <td>   -0.203</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:             many_bikes   No. Observations:                 1771\n",
       "Model:                          Logit   Df Residuals:                     1766\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Sun, 29 Jan 2023   Pseudo R-squ.:                -0.08839\n",
       "Time:                        15:37:21   Log-Likelihood:                -904.02\n",
       "converged:                       True   LL-Null:                       -830.61\n",
       "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "near_restaurant      -0.4756      0.111     -4.281      0.000      -0.693      -0.258\n",
       "near_grocery         -0.5757      0.141     -4.090      0.000      -0.852      -0.300\n",
       "near_prof_service    -0.9162      0.102     -8.999      0.000      -1.116      -0.717\n",
       "near_firm            -0.5322      0.174     -3.066      0.002      -0.872      -0.192\n",
       "near_accountant      -0.5649      0.185     -3.056      0.002      -0.927      -0.203\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_result.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, this is a logistic regression classification model but not a good one. The negative pseudo r-squa means it performs worse than a null model. On the bright side the log-likelihood seems relatively low, compared to my last models at least. One neat thing is the coefs are all negative. This means the model would initially assume a higher total_bikes value then based on if these columns were true it would decrease that count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>many_bikes</td>    <th>  No. Observations:  </th>  <td>  1771</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1761</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 29 Jan 2023</td> <th>  Pseudo R-squ.:     </th> <td>-0.07314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:45:39</td>     <th>  Log-Likelihood:    </th> <td> -891.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -830.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_restaurant</th>           <td>   -0.3167</td> <td>    0.122</td> <td>   -2.599</td> <td> 0.009</td> <td>   -0.555</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_grocery</th>              <td>   -0.4919</td> <td>    0.144</td> <td>   -3.408</td> <td> 0.001</td> <td>   -0.775</td> <td>   -0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_prof_service</th>         <td>   -0.8240</td> <td>    0.109</td> <td>   -7.586</td> <td> 0.000</td> <td>   -1.037</td> <td>   -0.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_hair_salon</th>           <td>   -0.3390</td> <td>    0.142</td> <td>   -2.389</td> <td> 0.017</td> <td>   -0.617</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_tea_and_coffee</th>       <td>   -0.3060</td> <td>    0.148</td> <td>   -2.072</td> <td> 0.038</td> <td>   -0.595</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_firm</th>                 <td>   -0.5296</td> <td>    0.176</td> <td>   -3.005</td> <td> 0.003</td> <td>   -0.875</td> <td>   -0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_accountant</th>           <td>   -0.5633</td> <td>    0.186</td> <td>   -3.024</td> <td> 0.002</td> <td>   -0.928</td> <td>   -0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_club</th>                 <td>   -0.3594</td> <td>    0.164</td> <td>   -2.187</td> <td> 0.029</td> <td>   -0.681</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_community_government</th> <td>    0.4931</td> <td>    0.198</td> <td>    2.486</td> <td> 0.013</td> <td>    0.104</td> <td>    0.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>near_language_school</th>      <td>    1.2517</td> <td>    0.610</td> <td>    2.052</td> <td> 0.040</td> <td>    0.056</td> <td>    2.447</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:             many_bikes   No. Observations:                 1771\n",
       "Model:                          Logit   Df Residuals:                     1761\n",
       "Method:                           MLE   Df Model:                            9\n",
       "Date:                Sun, 29 Jan 2023   Pseudo R-squ.:                -0.07314\n",
       "Time:                        15:45:39   Log-Likelihood:                -891.36\n",
       "converged:                       True   LL-Null:                       -830.61\n",
       "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "near_restaurant              -0.3167      0.122     -2.599      0.009      -0.555      -0.078\n",
       "near_grocery                 -0.4919      0.144     -3.408      0.001      -0.775      -0.209\n",
       "near_prof_service            -0.8240      0.109     -7.586      0.000      -1.037      -0.611\n",
       "near_hair_salon              -0.3390      0.142     -2.389      0.017      -0.617      -0.061\n",
       "near_tea_and_coffee          -0.3060      0.148     -2.072      0.038      -0.595      -0.017\n",
       "near_firm                    -0.5296      0.176     -3.005      0.003      -0.875      -0.184\n",
       "near_accountant              -0.5633      0.186     -3.024      0.002      -0.928      -0.198\n",
       "near_club                    -0.3594      0.164     -2.187      0.029      -0.681      -0.037\n",
       "near_community_government     0.4931      0.198      2.486      0.013       0.104       0.882\n",
       "near_language_school          1.2517      0.610      2.052      0.040       0.056       2.447\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_result.summary()\n",
    "# Here's another one with a p-value threshold at <0.05\n",
    "# Different but nothing to write home about."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a34b3273de5bc47428473d2a098b2b644951547a78dfc045c680f0cd3ff6f5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
